[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Data Scientist. R Enthusiast."
  },
  {
    "objectID": "drafts/numbats/index.html",
    "href": "drafts/numbats/index.html",
    "title": "Spatial Temporal Visualization of Numbat Sightings",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(echarts4r)\nlibrary(echarts4r.maps)\nlibrary(ggmap)\nlibrary(leaflet)\n\n\nIn this post I will leverage some unique capabilities from the {echarts4r} package to visualize both spatial and temporal data at the same time. Normally, you would need more than 1 plot to accomplish this, but {echarts4r} has a nifty “timeline” feature that makes it effortless to combine the two.\nI will use the #tidytuesday data set from March 7, 2023. The data consists of sightings of the endangered Numbat, a carnivorous marsupial native to Australia. It’s closest relative is the Tasmanian Devil. Sightings date back to 1856.\n\n\nCode\nnumbats <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-03-07/numbats.csv')\n\n\nFirst, let’s explore where Numbats have been spotted without respect to any time variable.\n\n\nCode\nnumbats |>\n  drop_na(month) |> \n  leaflet() |>\n  addTiles() |> \n  addCircleMarkers(\n    lng = ~decimalLongitude,\n    lat = ~decimalLatitude,\n    radius = 2\n  )\n\n\n\n\n\n\n\n\nCode\nnumbats |> \n  drop_na(month) |> \n  count(month = factor(month, levels = month.abb)) |>\n  ggplot(aes(month, n)) +\n  # geom_point() +\n  geom_col()\n\n\n\n\n\n\n\nCode\nnumbats |> \n  drop_na(month) |> \n  mutate(\n    decimalLongitude = signif(decimalLongitude, digits = 2),\n    decimalLatitude = signif(decimalLatitude, digits = 2)\n  ) |>\n  add_count(month, decimalLongitude, decimalLatitude) |>\n  distinct(month, decimalLatitude, decimalLongitude, .keep_all = T) |> \n  group_by(month = factor(month, levels = month.abb)) |>\n  e_charts(\n    decimalLongitude,\n    timeline = T\n  ) |>\n  # e_leaflet() |> \n  # e_leaflet_tile() |> \n  e_geo(\n    roam = TRUE,\n    boundingCoords = list(\n      c(140, -10),\n      c(120, -40)\n    )\n  ) |>\n  e_scatter(\n    decimalLatitude, \n    n,\n    coord_system = \"geo\",\n    symbol = \"pin\",\n  ) |> \n  # e_labels() |>\n  e_timeline_opts(\n    axis_type = \"category\",\n    playInterval = 1000,\n    realtime = F\n  ) |>\n  e_animation(duration.update = 100, delay.update = 0) |> \n  e_visual_map(min = 1, max = 20, inRange = list(color = c(\"rgb(253,231,37)\", \"rgb(68,1,84)\"), symbolSize = c(2, 60)))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Reproducing USDA Annual Utah Snowpack Plot\n\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2023\n\n\nParker Barnes\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Survival Times from Alone\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2023\n\n\nParker Barnes\n\n\n\n\n\n\n  \n\n\n\n\nDecline of Big Tech Stock\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 9, 2023\n\n\nParker Barnes\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/alone/index.html#introduction",
    "href": "posts/alone/index.html#introduction",
    "title": "Predicting Survival Times from Alone",
    "section": "Introduction",
    "text": "Introduction\nToday I’ll explore another interesting #tidytuesday data set consisting of data from the TV show Alone. For those not familiar with the show, 10 contestants are dropped off in a remote location in the wilderness with limited supplies. They attempt to survive for as long as they can until they tap out, and the last man standing wins a significant cash prize.\nThe data has its own R package. The package as well as more information about the data can be found here.\nThe goal of this analysis is to explore and model the factors that predict how long a competitor will survive."
  },
  {
    "objectID": "posts/alone/index.html#data-exploration",
    "href": "posts/alone/index.html#data-exploration",
    "title": "Predicting Survival Times from Alone",
    "section": "Data Exploration",
    "text": "Data Exploration\n\nData Ingest\nFirst let’s import the data for our survivalists and the locations\n\n\nCode\nlibrary(tidyverse)\nlibrary(alone)\n\ndata(survivalists)\ndata(seasons)\n\nglimpse(survivalists)\n\n\nRows: 94\nColumns: 19\n$ version             <chr> \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"U…\n$ season              <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2,…\n$ id                  <chr> \"US010\", \"US009\", \"US008\", \"US007\", \"US006\", \"US00…\n$ name                <chr> \"Alan Kay\", \"Sam Larson\", \"Mitch Mitchell\", \"Lucas…\n$ age                 <dbl> 40, 22, 34, 32, 37, 44, 46, 24, 41, 31, 50, 44, 45…\n$ gender              <chr> \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"M…\n$ city                <chr> \"Blairsville\", \"Lincoln\", \"Bellingham\", \"Quasqueto…\n$ state               <chr> \"Georgia\", \"Nebraska\", \"Massachusetts\", \"Iowa\", \"P…\n$ country             <chr> \"United States\", \"United States\", \"United States\",…\n$ result              <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7…\n$ days_lasted         <dbl> 56, 55, 43, 39, 8, 6, 4, 4, 1, 0, 66, 64, 59, 57, …\n$ medically_evacuated <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, F…\n$ reason_tapped_out   <chr> NA, \"Lost the mind game\", \"Realized he should actu…\n$ reason_category     <chr> NA, \"Family / personal\", \"Family / personal\", \"Fam…\n$ team                <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ day_linked_up       <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ profession          <chr> \"Corrections Officer\", \"Outdoor Gear Retailer\", \"B…\n$ url                 <chr> \"alan-kay\", \"sam-larson\", \"mitch-mitchell\", \"lucas…\n$ image_url           <chr> \"2020/04/Alone_Alan_Kay_Bio.jpg?w=840\", \"2020/04/A…\n\n\ndays_lasted is the variable we are most interested in and the one we will try to predict. Furthermore, we aren’t interested in any variable that isn’t known at the start of the season (i.e. reason_tapped_out)\n\n\nGender\nHow many men vs. women have attempted the show?\n\n\nCode\nsurvivalists |> count(gender)\n\n\n# A tibble: 2 × 2\n  gender     n\n  <chr>  <int>\n1 Female    20\n2 Male      74\n\n\nLooks like there are about 4 times as many men as there are women. How do men and women compare in their survival times?\n\n\nCode\nsurvivalists |> \n  ggplot(aes(gender, days_lasted)) +\n  geom_boxplot(aes(fill = gender), show.legend = F) +\n  geom_jitter(width = .2, height = 0) +\n  coord_flip() +\n  labs(x = NULL, y = \"Days Lasted\", fill = NULL)\n\n\n\n\n\nWomen in this show appear to have a slight survival advantage.\n\n\nLocation\nHow do survival times compare by season and location?\n\n\nCode\nsurvivalists |> \n  left_join(seasons |> select(season, location)) |> \n  mutate(season = fct_reorder(factor(season), -season)) |> \n  ggplot(aes(season, days_lasted, fill = location)) +\n  geom_boxplot(alpha = .5) +\n  geom_jitter(width = .2, height = 0) +\n  coord_flip() +\n  labs(x = \"Season\", y = \"Days Lasted\", fill = NULL)\n\n\n\n\n\nIf you look closely, you’ll notice that season 4 competitors drop out in pairs. That’s because the competitors were put in teams in this season. We’ll be sure to account for that when we model.\nFinally, let’s compare gender survival times by location\n\n\nCode\nsurvivalists |> \n  left_join(seasons |> select(season, location)) |> \n  ggplot(aes(location, days_lasted, fill = gender)) +\n  geom_boxplot(alpha = .5) +\n  geom_jitter(aes(color = gender), width = .2, height = 0, show.legend = F) +\n  coord_flip() +\n  labs(x = NULL, y = \"Days Lasted\", fill = NULL, color = NULL)\n\n\n\n\n\nFor some locations, women greatly out-survive men. Others, not so much.\n\n\nAge\nHow does age correlate with survival?\n\n\nCode\nsurvivalists |> \n  ggplot(aes(age, days_lasted, color = gender)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", lty = 2) +\n  labs(x = \"Age\", y = \"Days Lasted\", color = NULL) +\n  ggpubr::stat_cor(show.legend = F) \n\n\n\n\n\nThere doesn’t appear to be an age effect.\n\n\nItems\nLet’s import one more data set that consists of the items the survivalists bring with them on the show.\n\n\nCode\ndata(loadouts)\nlibrary(patchwork)\n\nridges <- \n  loadouts |>\n  left_join(survivalists |> select(name, season, days_lasted)) |>\n  add_count(item) |>\n  mutate(\n    item = fct_lump_min(item, min = 3),\n    item = fct_reorder(item, n)\n  ) |>\n  ggplot(aes(days_lasted, item)) +\n  ggridges::geom_density_ridges(aes(fill = item), panel_scaling = F, show.legend = F, alpha = .8) +\n  labs(x = \"Days Lasted\", y = NULL)\n\ntotals <-\n  loadouts |> \n  mutate(item = fct_lump_min(item, min = 3)) |>\n  count(item) |> \n  mutate(item = fct_reorder(item, n)) |> \n  mutate(item = fct_relevel(item, \"Other\", after = 0)) |> \n  ggplot(aes(item, n)) +\n  geom_col(alpha = .8, aes(fill = item), show.legend = F) +\n  geom_label(aes(label = n), size = 3) +\n  coord_flip() +\n  labs(y = \"Total\", x = NULL) +\n  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank())\n\nridges + totals\n\n\n\n\n\nWhile it’s difficult to infer anything from these charts, there is evidence that items do have an effect on survival.\n\n\nRepeated Contestants\nWe should also investigate the fact that in season 5, each contestant was a repeat survivalist. We should look at how each of them fared from their first attempt to their second.\n\n\nCode\nsurvivalists |> \n  left_join(seasons |> select(season, location)) |> \n  filter(n() > 1, .by = name) |> \n  mutate(\n    attempt = str_glue(\"Attempt {rank(season)}\"), \n    slope = (days_lasted[attempt == \"Attempt 1\"] - days_lasted[attempt == \"Attempt 2\"]) / 2,\n    .by = name, days_lasted, location, .keep = \"used\"\n  ) |>\n  ggplot(aes(attempt, days_lasted, group = name)) + \n  geom_point(aes(shape = location), size = 2) +\n  geom_line(aes(color = slope > 0)) +\n  scale_color_manual(values = c(\"#00b545\", \"red\"), labels = c(\"Survived Longer\", \"Survived Shorter\")) +\n  labs(title = \"Repeat Survivalists\", x = NULL, y = \"Days Lasted\", shape = \"Location\", color = \"2nd Attempt\")\n\n\n\n\n\nIt’s interesting that only roughly half of them survived longer on their 2nd attempt even accounting for location of their first attempt.\n\n\nPrior Survival Experience\nLastly, let’s look at how profession might affect survival time. Some contestants are “Survival Experts” or “Hunting Guides”. So we will label them to have “Prior Experience”.\n\n\nCode\nsurvivalists |> \n  mutate(prior_experience = profession |> str_detect(\"Surv|Wilderness|Hunt|Outdoor|Skills|Instructor\")) |> \n  ggplot(aes(prior_experience, days_lasted)) + \n  geom_boxplot(aes(fill = prior_experience), alpha = .5) +\n  ggrepel::geom_text_repel(\n    aes(label = profession), \n    direction = \"y\",\n    min.segment.length = 50,  \n    max.overlaps = 50, \n    position = position_jitter(width = .2, height = 0), \n    size = 2\n  ) +\n  coord_flip() +\n  labs(x = \"Prior Experience\", y = \"Days Lasted\") +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/alone/index.html#machine-learning",
    "href": "posts/alone/index.html#machine-learning",
    "title": "Predicting Survival Times from Alone",
    "section": "Machine Learning",
    "text": "Machine Learning\nLet’s build a model to predict survival time!\nFirst, we will prepare our data by selecting the necessary variables. We’ll have to pivot the items data to get it into wide format so we can join with our survivalists data set.\n\n\nCode\nitems_wide <- \n  loadouts |> \n  mutate(item = fct_lump_min(item, min = 3)) |>\n  mutate(present = 1) |> \n  pivot_wider(\n    id_cols = c(name, season), \n    names_from = item, \n    values_from = present,\n    values_fn = first,\n    values_fill = 0,\n    names_prefix = \"item_\"\n  )\n\nalone_clean <-\n  survivalists |> \n  left_join(seasons |> select(season, location)) |> \n  left_join(items_wide) |> \n  mutate(repeat_attempt = rank(season) > 1, .by = name) |> \n  select(\n    name, season, age, gender, location, days_lasted, team, country, profession, repeat_attempt, \n    starts_with(\"item_\")\n  )\n\n\n\nTrain/Test Sets\nNext we will create our train/test sets, as well as some resamples. Since our data set is so small, we will use bootstrap resampling and we will stratify by season to ensure we sample across different survival conditions.\n\n\nCode\nlibrary(tidymodels)\n\nset.seed(234)\nalone_split <- initial_split(alone_clean, strata = season)\nalone_train <- training(alone_split)\nalone_test <- testing(alone_split)\n\nalone_boot <- bootstraps(alone_train, times = 10, strata = season)\n\n\n\n\nFeature Engineering\nMoving on to our feature engineering, we will create a few of the variables we’ve already explored.\nOne of my favorite pre-processing techniques to use when data includes a grouping structure is step_lencode_mixed() from the {embed} package. This step assigns a numeric value to our location variable by training a simple mixed model using the {lme4} package. This is better than dummy encoding because it accounts for differing group counts and will automatically assign a value close to the mean when a new location is encountered. Plus, it doesn’t increase the number of dimensions of the data set as dummy encoding would.\n\n\nCode\nalone_rec <- \n  recipe(\n    days_lasted ~ .,\n    data = alone_train\n  ) |> \n  update_role(name, new_role = \"id\") |> \n  update_role(season, new_role = \"season_id\") |> \n  step_mutate(\n    prior_experience = profession |> stringr::str_detect(\"Surv|Wilderness|Hunt|Outdoor|Skills|Instruct\"),\n    US_orig = country == \"United States\",\n    team = !is.na(team),\n    across(where(is.logical), factor)\n  ) |> \n  embed::step_lencode_mixed(location, outcome = vars(days_lasted)) |> \n  step_rm(profession, country) |> \n  step_dummy(all_factor_predictors())\n\n\n\n\nModel Training/Tuning\nLet’s train a random forest regression model. We will tune and optimize for RMSE. Since it’s a small data set, we will stick with relatively small numbers of trees.\n\n\nCode\nrf_spec <- \n  rand_forest(mtry = tune(), trees = tune()) |>\n  set_engine(\"ranger\") |> \n  set_mode(\"regression\")\n\nalone_wf <-\n  workflow() |>\n  add_recipe(alone_rec) |>\n  add_model(rf_spec)\n\nrf_grid <- \n  grid_regular(\n    mtry(c(5, 25)), \n    trees(c(10, 100)), \n    levels = c(mtry = 5, trees = 10)\n  )\n\n\n\n\nCode\ndoParallel::registerDoParallel()\n\nalone_res <-\n  alone_wf |>\n  tune_grid(\n    resamples = alone_boot,\n    grid = rf_grid,\n    control = control_resamples(allow_par = T, parallel_over = \"everything\")\n  )\n\n\n\n\nCode\nalone_res |> autoplot()\n\n\n\n\n\nTurns out this is a very hard problem to predict. Nevertheless, we will pick our best model and do one last fit.\n\n\nCode\nbest_params <- alone_res |> select_best(metric = \"rmse\")\n\nset.seed(678)\n\nalone_final <- \n  alone_wf |> \n  update_model(rand_forest(mode = \"regression\") |> set_engine(\"ranger\", importance = \"permutation\")) |>\n  finalize_workflow(best_params) |> \n  last_fit(alone_split)"
  },
  {
    "objectID": "posts/alone/index.html#results",
    "href": "posts/alone/index.html#results",
    "title": "Predicting Survival Times from Alone",
    "section": "Results",
    "text": "Results\n\nFinal Performance Metrics\n\n\nCode\nalone_final |> collect_metrics()\n\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard      27.3   Preprocessor1_Model1\n2 rsq     standard       0.186 Preprocessor1_Model1\n\n\nLooks like the final model’s metrics were slightly better than our cross-validated metrics. This is probably because we got to train on the full training set.\n\n\nPredictions\nNow we will compare our predictions against the true values from the test set. The dotted black line represents the formula y = x, which is where we’d hope all our predictions would lie.\n\n\nCode\nalone_final |> \n  collect_predictions() %>%\n  inner_join(alone_clean |> slice(.$.row)) |> \n  ggplot(aes(.pred, days_lasted)) +\n  geom_point(aes(color = location, shape = location), size = 2) +\n  geom_abline(slope = 1, intercept = 0, lty = 2) +\n  labs(shape = NULL, color = NULL, x = \"Predicted Days Lasted\", y = \"Actual Days Lasted\") +\n  expand_limits(x = 0, y = 0)\n\n\n\n\n\nThe good news is that there doesn’t appear to be a whole lot of bias in our model (no systematic under or over predicting). However, our predictions tend to be quite far off most of the time.\n\n\nVariable Importance\nTo finish things off, we’ll look at the variable importances tracked by our model. It’s important to note that permutation importance doesn’t indicate direction of effect, only magnitude. Nonetheless, we can infer the probable direction by looking back at our EDA plots.\n\n\nCode\nalone_final |> \n  extract_fit_engine() |>\n  vip::vip(n = 20)\n\n\n\n\n\nUnsurprisingly we see that gender and location affect survival time. Interestingly, there are a few not so obvious items that appear to affect survival time. Perhaps trapping wire is a must to survive in the wild!"
  },
  {
    "objectID": "posts/alone/index.html#conclusion-discussion",
    "href": "posts/alone/index.html#conclusion-discussion",
    "title": "Predicting Survival Times from Alone",
    "section": "Conclusion & Discussion",
    "text": "Conclusion & Discussion\nAfter exploring and modeling this data set, it’s clear that it’s quite difficult to predict how long contestants will last on the show. There are likely two main reasons for this. The first is that our sample is so limited. 9 seasons with 10 contestants each is no where near enough to extract robust and meaningful patterns. The second reason is that we don’t have enough data on the existing contestants. Additional information that could be helpful include relationship status/family info, personality characteristics, and socio-economic status.\nMore detailed data about the locations could be helpful as well, such as time of year and weather patterns at drop-off time. Perhaps some of this information could be acquired from actually watching the show.\nThere’s also the fact that there is likely some inconsistent bias that the producers of the show introduce when they select contestants. It’s very possible that they select people differently each season based on how viewership and popularity of prior seasons. This makes it difficult to detect potential patterns across seasons."
  },
  {
    "objectID": "posts/big-tech-stock/index.html",
    "href": "posts/big-tech-stock/index.html",
    "title": "Big Tech Stock: Per-operation Grouping",
    "section": "",
    "text": "Welcome to my very first #tidytuesday blog post! In this post I will showcase the new per-operation grouping functionality released in dplyr 1.1.0. I also want to demonstrate one of my favorite lesser-known dplyr tricks!\nI’ll use this week’s tidytuesday data set consisting of daily big tech stock prices from 2010-2022 to demonstrate.\nFirst, let’s download the data:\n\nlibrary(tidyverse, quietly = T)\n\nbig_tech_stock_prices <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-02-07/big_tech_stock_prices.csv')\nbig_tech_companies <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-02-07/big_tech_companies.csv')\n\nbig_tech_stock_prices |> glimpse()\n\nRows: 45,088\nColumns: 8\n$ stock_symbol <chr> \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"…\n$ date         <date> 2010-01-04, 2010-01-05, 2010-01-06, 2010-01-07, 2010-01-…\n$ open         <dbl> 7.622500, 7.664286, 7.656429, 7.562500, 7.510714, 7.60000…\n$ high         <dbl> 7.660714, 7.699643, 7.686786, 7.571429, 7.571429, 7.60714…\n$ low          <dbl> 7.585000, 7.616071, 7.526786, 7.466071, 7.466429, 7.44464…\n$ close        <dbl> 7.643214, 7.656429, 7.534643, 7.520714, 7.570714, 7.50392…\n$ adj_close    <dbl> 6.515213, 6.526476, 6.422664, 6.410790, 6.453412, 6.39648…\n$ volume       <dbl> 493729600, 601904800, 552160000, 477131200, 447610800, 46…\n\nbig_tech_stock_prices |> \n  count(stock_symbol) |> \n  inner_join(big_tech_companies) |> \n  select(company, stock_symbol, n)\n\n# A tibble: 14 × 3\n   company                                     stock_symbol     n\n   <chr>                                       <chr>        <int>\n 1 Apple Inc.                                  AAPL          3271\n 2 Adobe Inc.                                  ADBE          3271\n 3 Amazon.com, Inc.                            AMZN          3271\n 4 Salesforce, Inc.                            CRM           3271\n 5 Cisco Systems, Inc.                         CSCO          3271\n 6 Alphabet Inc.                               GOOGL         3271\n 7 International Business Machines Corporation IBM           3271\n 8 Intel Corporation                           INTC          3271\n 9 Meta Platforms, Inc.                        META          2688\n10 Microsoft Corporation                       MSFT          3271\n11 Netflix, Inc.                               NFLX          3271\n12 NVIDIA Corporation                          NVDA          3271\n13 Oracle Corporation                          ORCL          3271\n14 Tesla, Inc.                                 TSLA          3148\n\n\nNote: Meta and Tesla did not go public until after Jan 2010, and so they have a few less observations\nNow let’s visualize the stocks as simple faceted line charts\n\nbig_tech_stock_prices |>\n  ggplot(aes(date, adj_close)) +\n  geom_line() +\n  facet_wrap(~stock_symbol, ncol = 4, scales = \"free_y\") +\n  labs(x = NULL, y = NULL)\n\n\n\n\nWith the exception of IBM, each stock peaks around the end of 2021, and then declines thereafter.\nLet’s zoom in on the peaks by plotting them on the same axes. Here I will use the new dplyr functionality to find the maximum adj_close by stock_symbol.\n\nbig_tech_stock_prices |> \n  filter(stock_symbol != \"IBM\") |> \n  slice_max(adj_close, by = stock_symbol) |> \n  ggplot(aes(date, adj_close, color = stock_symbol)) +\n  geom_point() +\n  ggrepel::geom_text_repel(aes(label = stock_symbol), size = 3, vjust = -.75) +\n  scale_x_date(labels = scales::label_date_short(), breaks = \"month\") +\n  labs(title = \"Peak Stock Prices\", x = NULL, y = NULL) +\n  theme(legend.position = \"none\")\n\n\n\n\nHow steep of a decline do these stocks see in the weeks and months following their peak? How do they compare to each other?\nTo answer this question, we will need to filter each stock to include only the data following its peak. This may seem trivial at first, but it’s a bit trickier than you think. Since each stock has a different date at which is hits its peak price, we can’t simply filter by a single date.\nOne approach would be to save the dates at which the stocks hit their maximum prices into a separate tibble, join that tibble with the original, and finally, filter the dates.\n\npeak_price_dates <- \n  big_tech_stock_prices |> \n  slice_max(adj_close, by = stock_symbol) |> \n  select(stock_symbol, peak_date = date, peak_price = adj_close)\n\nbig_tech_stock_prices |> \n  inner_join(peak_price_dates) |> \n  filter(date >= peak_date) |> \n  # for demonstration purposes\n  slice_min(date, n = 3, by = stock_symbol) |> \n  select(stock_symbol, date, adj_close)\n\n# A tibble: 42 × 3\n   stock_symbol date       adj_close\n   <chr>        <date>         <dbl>\n 1 AAPL         2022-01-03      181.\n 2 AAPL         2022-01-04      179.\n 3 AAPL         2022-01-05      174.\n 4 ADBE         2021-11-19      688.\n 5 ADBE         2021-11-22      674.\n 6 ADBE         2021-11-23      665.\n 7 AMZN         2021-07-08      187.\n 8 AMZN         2021-07-09      186.\n 9 AMZN         2021-07-12      186.\n10 CRM          2021-11-08      310.\n# … with 32 more rows\n\n\nThis solution works, but there’s a better (in my opinion) way that doesn’t require a separate tibble. The method is derived from a base-R concept called subsetting.\nSubsetting can be used to filter a vector or dataframe by some condition, much like dplyr::filter. Instead of a function call, we use square brackets ([]).\n\nvec <- 0:20\n# subset to get even numbers\nvec[vec %% 2 == 0]\n\n [1]  0  2  4  6  8 10 12 14 16 18 20\n\n# subset to get rows with mpg > 18\nmtcars[mtcars$mpg > 21,]\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nDatsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nMerc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\nIn dplyr, we can apply this same principle by combining a filter with a subset. For our case, we combine filter, subset, and which.max to isolate the max price dates. Coupled with per-operation grouping, we can accomplish the entire process in a single step!\n\nstock_downfalls <- \n  big_tech_stock_prices |>\n  # filter date by the grouped and subsetted date\n  filter(date >= date[which.max(adj_close)], .by = stock_symbol)\n\nstock_downfalls |> \n  ggplot(aes(date, adj_close, color = stock_symbol)) +\n  geom_line() +\n  ggrepel::geom_label_repel(aes(peak_date, peak_price, label = stock_symbol), data = peak_price_dates, size = 3) +\n  scale_x_date(labels = scales::label_date_short(), breaks = \"month\") +\n  labs(title = \"Big Tech Stock Declines\", x = NULL, y = NULL) +\n  theme(legend.position = \"none\")\n\n\n\n\nNow we can clearly see which stocks endured more dramatic price dips and how they fared over the following 1-2 years.\nThank you so much for reading and I hope this exercise was useful. Please reach out if you have any questions or feedback!"
  },
  {
    "objectID": "posts/snow/index.html",
    "href": "posts/snow/index.html",
    "title": "Reproducing USDA Annual Utah Snowpack Plot",
    "section": "",
    "text": "Sundance, UT. Photo graciously provided by my wife Kelsey Barnes (:\n\n\n\nIntro\nAs of March 24, Utah broke a 40 year old record for largest snowpack in 40 years. There has been a beautiful interactive chart floating around the internet provided by the USDA that effectively shows yearly snow water accumulation over the years. I thought it would be fun to attempt to reproduce the plot using R.\n\n\n\nScreenshot of the USDA Snowpack Plot\n\n\n\n\nData\n\n\nCode\n# functions we'll need for this analysis\nbox::use(\n  readr[read_csv],\n  tidyr[pivot_longer],\n  DT[datatable],\n  dplyr[...],\n  ggplot2[...],\n  lubridate[as_date, yday, today, month, mday],\n  scales[label_date],\n  stringr[str_glue]\n)\n\n\nOn the main page where the chart is hosted, there is a link to the source of the data, making it super convenient to pull the up-to-date data used to create the chart. I’ve also included a link to it here.\nThis type of data source is a web-hosted csv file. The {readr} package can handle this file just as it would any other csv.\n\nsnow <- read_csv(\"https://www.nrcs.usda.gov/Internet/WCIS/AWS_PLOTS/basinCharts/POR/WTEQ/assocHUCut3/state_of_utah.csv\")\n\n\n\nPivoting\nLet’s take a look at the structure of data.\n\n\nCode\ndatatable(snow)\n\n\n\n\n\n\n\nIn this format, each year gets its own column (wide format). While this is a nice way to view the data, it’s not ideal for plotting. We need to “tidy” the data by pivoting to a longer format. The pivot_longer function from the {tidyr} package makes this task effortless.\n\n\nCode\nsnow_long <- \n  snow |> \n  pivot_longer(\n    cols = `1981`:`2023`, \n    names_to = \"year\", \n    values_to = \"snowpack\", \n    values_drop_na = T\n  ) |> \n  select(date, year, snowpack)\n\ndatatable(snow_long)\n\n\n\n\n\n\n\n\n\nPlotting with ggplot2\nNow that the data are in “long” format, let’s attempt to make a basic visualization.\n\n\n\n\n\n\nNote\n\n\n\nThe variable snowpack is in terms of inches of snow water equivalent. For simplicity’s sake, I’ll just continue calling it snowpack.\n\n\n\n\nCode\nsnow_long |> \n  ggplot(aes(date, snowpack, color = year)) + \n  geom_line()\n\n\n\n\n\nWhat happened? The reason it looks like this is because our date variable isn’t the appropriate type.\n\n\nCode\nstr(snow_long$date)\n\n\n chr [1:15525] \"10-01\" \"10-01\" \"10-01\" \"10-01\" \"10-01\" \"10-01\" \"10-01\" ...\n\n\nWe need to convert date from a chr to a date using the as_date function from {lubridate}. We will add a simple mutate to do the conversion and feed the data straight back into a ggplot.\n\n\nCode\nsnow_long |> \n  mutate(date = as_date(date, format = \"%m-%d\")) |> \n  ggplot(aes(date, snowpack, color = year)) + \n  geom_line()\n\n\n\n\n\nThis is looking better, but something is still off. If you look back at the original plot, the data should start in October, not January. The reason this happened is that when we converted the date variable to a date type, we didn’t account for the year. We need to account for the fact that the data starts in October and rolls into the next year.\nThe easiest way to do this in my opinion is to first extract the day of the year from the date, and then add that number of days to Jan 1 of either the “first year” or the “second year”, based on the “threshold” day (Oct 1). In other words, any day after Oct 1 (day 275) should be in the “first year” and every other day should be assigned to the “second year”.\nDon’t worry that I picked 2021 and 2022 for my first and second years, we will ignore year for now and deal with it later.\n\n\nCode\nsnow_temp <- \n  snow_long |> \n  mutate(\n    date = as_date(date, format = \"%m-%d\") |> yday(),\n    date = if_else(date >= 275, as_date(\"2021-01-01\") + date, \n                   as_date(\"2022-01-01\") + date)\n  )\n\nsnow_plot <-\n  snow_temp |> \n  ggplot(aes(date, snowpack, color = year)) +\n  geom_line() +\n  scale_x_date(labels = label_date(\"%b\"), breaks = \"month\") +\n  labs(\n    title = \"Utah Snowpack\",\n    x = NULL,\n    y = \"Snow Water Equivalent (in.)\",\n    color = NULL\n  )\n\nsnow_plot\n\n\n\n\n\n\n\nPlotting with Plotly\nExcellent! Our plot is accurately showing the data as in the USDA original! This is still kind of an ugly plot however. Even with a legend, it’s hard to know which line corresponds with which year. The original chart allowed users to interact with the data and select which years they wanted to show.\nLucky for us, there is a super quick hack that will turn our plot interactive in a single function call! The function comes from the {plotly} package which provides an api to the plotting software used on original USDA chart.\n\n\nCode\nbox::use(plotly[ggplotly])\n\nggplotly(snow_plot)\n\n\n\n\n\n\nNow we have nice tooltips and a legend that let’s us filter which years we want to view. Try it yourself by double clicking on 1983 in the legend to deselect everything except that year, and then pick 2023 to compare the top 2 years for snowpack.\nLet’s make one more fix to make our plot more accurate. You’ll notice that we still see 2021 and 2022 as the years for the date in the tooltip. This is because plotly automatically pulls the raw value regardless of how we changed the axis labels. To fix this, we can format a text variable to make a more attractive tooltip, and then pass that in to our call to ggplotly.\n\n\nCode\nsnow_plot2 <- \n  snow_temp |> \n  mutate(\n    tooltip = str_glue(\n      \"{month(date, label = T, abbr = T)} {mday(date)}, {year}\n       Snowpack (in): {snowpack}\"\n    )\n  ) |> \n  ggplot(aes(date, snowpack, color = year, text = tooltip, group = year)) +\n  geom_line() +\n  scale_x_date(labels = label_date(\"%b\"), breaks = \"month\") +\n  labs(\n    title = \"Utah Snowpack\",\n    x = NULL,\n    y = \"Snow Water Equivalent (in.)\",\n    color = NULL\n  )\n\nggplotly(snow_plot2, tooltip = \"text\")\n\n\n\n\n\n\n\n\nBonus 1 (Plotting Function)\nLet’s say we wanted to recreate the plot daily so that it reflects the most recent data. We should probably capture the whole process in a single function. I will demonstrate that below, with a minor change. We will transform our date variable before we pivot the data, to be slightly more efficient. We can also add in function parameters, to make the plotting more flexible.\n\n\nCode\ncreate_snow_plot <- function(title = \"Utah Snowpack\", .interactive = T) {\n  \n  snow_raw <- read_csv(\"https://www.nrcs.usda.gov/Internet/WCIS/AWS_PLOTS/basinCharts/POR/WTEQ/assocHUCut3/state_of_utah.csv\") \n  \n  snow_plot <- \n    snow_raw |> \n    mutate(\n      date = as_date(date, format = \"%m-%d\") |> yday(),\n      date = if_else(date >= 275, as_date(\"2021-01-01\") + date, \n                     as_date(\"2022-01-01\") + date)\n    ) |> \n    pivot_longer(\n      `1981`:`2023`, names_to = \"year\", \n      values_to = \"snowpack\", values_drop_na = T\n    ) |> \n    mutate(\n      tooltip = str_glue(\n        \"{month(date, label = T, abbr = T)} {mday(date)}, {year}\n         Snowpack (in): {snowpack}\"\n      )\n    ) |> \n    ggplot(aes(date, snowpack, color = year, text = tooltip, group = year)) +\n    geom_line() +\n    scale_x_date(labels = label_date(\"%b\"), breaks = \"month\") +\n    labs(\n      title = title,\n      x = NULL,\n      y = \"Snow Water Equivalent (in.)\",\n      color = NULL\n    )\n  \n  if (.interactive) {\n    ggplotly(snow_plot, tooltip = \"text\")\n  } else {\n    snow_plot\n  }\n    \n}\n\ncreate_snow_plot(title = str_glue(\"Utah Snowpack as of {today()}\"))\n\n\n\n\n\n\n\n\nBonus 2 (echarts4r)\nLet’s recreate the plot one more time using another one of my favorite plotting packages, {echarts4r}. This package is simple to use and makes very attractive looking plots right out of the box. It also has a couple neat features that add unique interactivity capabilities to your charts.\n\n\n\n\n\n\nNote\n\n\n\nI’m going to forgo adding titles and axis labels, for the sake of simplicity. Those things are simple to implement however.\n\n\n\n\nCode\nbox::use(echarts4r[...])\n\nsnow_temp2 <- \n  snow_long |> \n  mutate(\n    day_of_year = as_date(date, format = \"%m-%d\") |> yday(),\n    year = as.numeric(year),\n    date = if_else(day_of_year >= 275, \n                   as_date(str_glue(\"{year - 1}-01-01\")) + day_of_year, \n                   as_date(str_glue(\"{year}-01-01\")) + day_of_year)\n  )\n\n\nsnow_temp2 |> \n  group_by(year) |>\n  e_charts(date, timeline = T) |> \n  e_area(snowpack) |> \n  e_tooltip(\"axis\") |> \n  e_datazoom() |> \n  e_legend(show = F) |> \n  e_timeline_opts(top = \"top\") |> \n  e_y_axis(max = max(snow_temp$snowpack, na.rm = T) + 3)\n\n\n\n\n\n\nInstead of plotting multiple years simultaneously, I’ve added a ‘timeline’ feature (top) to allow you to cycle through the data. There’s even an auto-play button on the left-hand side that when pressed, will cycle through the years automatically! Additionally, on the bottom there is a ‘data-zoom’ slider that lets you interactively zoom in on certain times of the year. Granted, this functionality is possible using plotly, but I like how clean it feels in this plot.\n\n\nBonus 3 (Daily Changes)\nSo far, these charts are effective at showing us cumulative snowpack over time, but what about the daily change in snow? Visually, we can estimate this value by looking at the change or slope between days. Wouldn’t it be nice if we could visualize the differences and plot them on the same axis as the original? Let’s see if we can do that.\nFirst, we need to do the calculation. We’ll use the lag function from {dplyr} to accomplish this.\n\n\nCode\nsnow_changes <- \n  snow_temp2 |> \n  mutate(daily_change = snowpack - lag(snowpack, order_by = date, default = 0)) |>\n  arrange(year, date) \n\nsnow_changes |> \n  select(snowpack, daily_change) |> \n  # for demonstration purposes\n  slice(200:205)\n\n\n# A tibble: 6 × 2\n  snowpack daily_change\n     <dbl>        <dbl>\n1      9.1       -0.5  \n2      8.8       -0.300\n3      8.7       -0.100\n4      8.6       -0.100\n5      8.4       -0.200\n6      8         -0.400\n\n\nThen we can make a new plot with our newly created daily_change variable. We then link the plots together using e_group and e_connect_group.\n\n\nCode\np1 <-\n  snow_changes |>\n  group_by(year) |> \n  e_charts(date, timeline = T, height = 300) |>\n  e_area(daily_change) |>\n  e_tooltip(trigger = \"axis\") |>\n  e_datazoom() |>\n  e_timeline_opts(show = F) |>\n  e_legend(right = \"right\", top = \"50%\") |>\n  e_group(\"snow\") |>\n  e_connect_group(\"snow\") |>\n  e_color(\"lightgreen\") |> \n  e_y_axis(max = max(snow_changes$daily_change, na.rm = T), min = min(snow_changes$daily_change, na.rm = T))\n\np2 <-\n  snow_changes |>\n  group_by(year) |> \n  e_charts(date, timeline = T, height = 300) |>\n  e_area(snowpack) |>\n  e_tooltip(trigger = \"axis\") |>\n  e_datazoom() |>\n  e_timeline_opts(top = \"top\") |>\n  e_legend(right = \"right\", top = \"50%\") |>\n  e_group(\"snow\") |>\n  e_y_axis(max = max(snow_changes$snowpack, na.rm = T) + 3)\n\ne_arrange(p1, p2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow we can clearly see which days saw lots of snowfall or big melts!\n\n\n\n\n\n\nTip\n\n\n\nIn 1983, Utah saw some of its worst flooding in recent history. See if you can use the chart above to see why!\n\n\n\n\nConclusion\nThere are numerous ways to make your plots look nicer and prettier. Admittedly, I’m not super into fancy plots myself, so I usually just go with whatever options are available “out of the box”. But I hope this tutorial gave you some practical ideas for using different plotting tools in R. Thanks for checking it out!"
  },
  {
    "objectID": "projects/alone_dashboard/index.html",
    "href": "projects/alone_dashboard/index.html",
    "title": "Exploring Alone Survivalists with {crosstalk} and {reactable}",
    "section": "",
    "text": "This is a super basic dashboard showcasing the data I explored in one of my recent blog posts. I mostly wanted to try out a few packages I had heard about but hadn’t used before. {flexdashboard} lets you create dashboards using RMarkdown files, and does not require Shiny. {crosstalk} adds basic interaction between dashboard elements. Again, without using shiny. And finally, {reactable} is a package for creating interactive tables, and it allowed me to incorporate nested tables for the survivalists’ items.\nI specifically used {flexdashboard} and {crosstalk} to avoid having to use Shiny, since it requires a server and a hosting service. I wanted to see if I could embed the dashboard directly in my website. Unfortunately, quarto does not support {flexdashboard} quite yet, but it’s in the works! So for now, I deployed my dashboard on RPubs, which is a free publishing platform supported by Posit.\nYou can access my app here: Alone Dashboard"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Exploring Alone Survivalists with {crosstalk} and {reactable}\n\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2023\n\n\nParker Barnes\n\n\n\n\n\n\nNo matching items"
  }
]