[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Data Scientist. R Enthusiast."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Predicting Survival Times from Alone\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2023\n\n\nParker Barnes\n\n\n\n\n\n\n  \n\n\n\n\nDecline of Big Tech Stock\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 9, 2023\n\n\nParker Barnes\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/alone/index.html#introduction",
    "href": "posts/alone/index.html#introduction",
    "title": "Predicting Survival Times from Alone",
    "section": "Introduction",
    "text": "Introduction\nToday I’ll explore another interesting #tidytuesday data set consisting of data from the TV show Alone. For those not familiar with the show, 10 contestants are dropped off in a remote location in the wilderness with limited supplies. They attempt to survive for as long as they can until they tap out, and the last man standing wins a significant cash prize.\nThe data has its own R package. The package as well as more information about the data can be found here.\nThe goal of this analysis is to explore and model the factors that predict how long a competitor will survive."
  },
  {
    "objectID": "posts/alone/index.html#data-exploration",
    "href": "posts/alone/index.html#data-exploration",
    "title": "Predicting Survival Times from Alone",
    "section": "Data Exploration",
    "text": "Data Exploration\n\nData Ingest\nFirst let’s import the data for our survivalists and the locations\n\n\nCode\nlibrary(tidyverse)\nlibrary(alone)\n\ndata(survivalists)\ndata(seasons)\n\nglimpse(survivalists)\n\n\nRows: 94\nColumns: 19\n$ version             <chr> \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"U…\n$ season              <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2,…\n$ id                  <chr> \"US010\", \"US009\", \"US008\", \"US007\", \"US006\", \"US00…\n$ name                <chr> \"Alan Kay\", \"Sam Larson\", \"Mitch Mitchell\", \"Lucas…\n$ age                 <dbl> 40, 22, 34, 32, 37, 44, 46, 24, 41, 31, 50, 44, 45…\n$ gender              <chr> \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"M…\n$ city                <chr> \"Blairsville\", \"Lincoln\", \"Bellingham\", \"Quasqueto…\n$ state               <chr> \"Georgia\", \"Nebraska\", \"Massachusetts\", \"Iowa\", \"P…\n$ country             <chr> \"United States\", \"United States\", \"United States\",…\n$ result              <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7…\n$ days_lasted         <dbl> 56, 55, 43, 39, 8, 6, 4, 4, 1, 0, 66, 64, 59, 57, …\n$ medically_evacuated <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, F…\n$ reason_tapped_out   <chr> NA, \"Lost the mind game\", \"Realized he should actu…\n$ reason_category     <chr> NA, \"Family / personal\", \"Family / personal\", \"Fam…\n$ team                <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ day_linked_up       <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ profession          <chr> \"Corrections Officer\", \"Outdoor Gear Retailer\", \"B…\n$ url                 <chr> \"alan-kay\", \"sam-larson\", \"mitch-mitchell\", \"lucas…\n$ image_url           <chr> \"2020/04/Alone_Alan_Kay_Bio.jpg?w=840\", \"2020/04/A…\n\n\ndays_lasted is the variable we are most interested in and the one we will try to predict. Furthermore, we aren’t interested in any variable that isn’t known at the start of the season (i.e. reason_tapped_out)\n\n\nGender\nHow many men vs. women have attempted the show?\n\n\nCode\nsurvivalists |> count(gender)\n\n\n# A tibble: 2 × 2\n  gender     n\n  <chr>  <int>\n1 Female    20\n2 Male      74\n\n\nLooks like there are about 4 times as many men as there are women. How do men and women compare in their survival times?\n\n\nCode\nsurvivalists |> \n  ggplot(aes(gender, days_lasted)) +\n  geom_boxplot(aes(fill = gender), show.legend = F) +\n  geom_jitter(width = .2, height = 0) +\n  coord_flip() +\n  labs(x = NULL, y = \"Days Lasted\", fill = NULL)\n\n\n\n\n\nWomen in this show appear to have a slight survival advantage.\n\n\nLocation\nHow do survival times compare by season and location?\n\n\nCode\nsurvivalists |> \n  left_join(seasons |> select(season, location)) |> \n  mutate(season = fct_reorder(factor(season), -season)) |> \n  ggplot(aes(season, days_lasted, fill = location)) +\n  geom_boxplot(alpha = .5) +\n  geom_jitter(width = .2, height = 0) +\n  coord_flip() +\n  labs(x = \"Season\", y = \"Days Lasted\", fill = NULL)\n\n\n\n\n\nIf you look closely, you’ll notice that season 4 competitors drop out in pairs. That’s because the competitors were put in teams in this season. We’ll be sure to account for that when we model.\nFinally, let’s compare gender survival times by location\n\n\nCode\nsurvivalists |> \n  left_join(seasons |> select(season, location)) |> \n  ggplot(aes(location, days_lasted, fill = gender)) +\n  geom_boxplot(alpha = .5) +\n  geom_jitter(aes(color = gender), width = .2, height = 0, show.legend = F) +\n  coord_flip() +\n  labs(x = NULL, y = \"Days Lasted\", fill = NULL, color = NULL)\n\n\n\n\n\nFor some locations, women greatly out-survive men. Others, not so much.\n\n\nAge\nHow does age correlate with survival?\n\n\nCode\nsurvivalists |> \n  ggplot(aes(age, days_lasted, color = gender)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", lty = 2) +\n  labs(x = \"Age\", y = \"Days Lasted\", color = NULL) +\n  ggpubr::stat_cor(show.legend = F) \n\n\n\n\n\nThere doesn’t appear to be an age effect.\n\n\nItems\nLet’s import one more data set that consists of the items the survivalists bring with them on the show.\n\n\nCode\ndata(loadouts)\nlibrary(patchwork)\n\nridges <- \n  loadouts |>\n  left_join(survivalists |> select(name, season, days_lasted)) |>\n  add_count(item) |>\n  mutate(\n    item = fct_lump_min(item, min = 3),\n    item = fct_reorder(item, n)\n  ) |>\n  ggplot(aes(days_lasted, item)) +\n  ggridges::geom_density_ridges(aes(fill = item), panel_scaling = F, show.legend = F, alpha = .8) +\n  labs(x = \"Days Lasted\", y = NULL)\n\ntotals <-\n  loadouts |> \n  mutate(item = fct_lump_min(item, min = 3)) |>\n  count(item) |> \n  mutate(item = fct_reorder(item, n)) |> \n  mutate(item = fct_relevel(item, \"Other\", after = 0)) |> \n  ggplot(aes(item, n)) +\n  geom_col(alpha = .8, aes(fill = item), show.legend = F) +\n  geom_label(aes(label = n), size = 3) +\n  coord_flip() +\n  labs(y = \"Total\", x = NULL) +\n  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank())\n\nridges + totals\n\n\n\n\n\nWhile it’s difficult to infer anything from these charts, there is evidence that items do have an effect on survival.\n\n\nRepeated Contestants\nWe should also investigate the fact that in season 5, each contestant was a repeat survivalist. We should look at how each of them fared from their first attempt to their second.\n\n\nCode\nsurvivalists |> \n  left_join(seasons |> select(season, location)) |> \n  filter(n() > 1, .by = name) |> \n  mutate(\n    attempt = str_glue(\"Attempt {rank(season)}\"), \n    slope = (days_lasted[attempt == \"Attempt 1\"] - days_lasted[attempt == \"Attempt 2\"]) / 2,\n    .by = name, days_lasted, location, .keep = \"used\"\n  ) |>\n  ggplot(aes(attempt, days_lasted, group = name)) + \n  geom_point(aes(shape = location), size = 2) +\n  geom_line(aes(color = slope > 0)) +\n  scale_color_manual(values = c(\"#00b545\", \"red\"), labels = c(\"Survived Longer\", \"Survived Shorter\")) +\n  labs(title = \"Repeat Survivalists\", x = NULL, y = \"Days Lasted\", shape = \"Location\", color = \"2nd Attempt\")\n\n\n\n\n\nIt’s interesting that only roughly half of them survived longer on their 2nd attempt even accounting for location of their first attempt.\n\n\nPrior Survival Experience\nLastly, let’s look at how profession might affect survival time. Some contestants are “Survival Experts” or “Hunting Guides”. So we will label them to have “Prior Experience”.\n\n\nCode\nsurvivalists |> \n  mutate(prior_experience = profession |> str_detect(\"Surv|Wilderness|Hunt|Outdoor|Skills|Instructor\")) |> \n  ggplot(aes(prior_experience, days_lasted)) + \n  geom_boxplot(aes(fill = prior_experience), alpha = .5) +\n  ggrepel::geom_text_repel(\n    aes(label = profession), \n    direction = \"y\",\n    min.segment.length = 50,  \n    max.overlaps = 50, \n    position = position_jitter(width = .2, height = 0), \n    size = 2\n  ) +\n  coord_flip() +\n  labs(x = \"Prior Experience\", y = \"Days Lasted\") +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/alone/index.html#machine-learning",
    "href": "posts/alone/index.html#machine-learning",
    "title": "Predicting Survival Times from Alone",
    "section": "Machine Learning",
    "text": "Machine Learning\nLet’s build a model to predict survival time!\nFirst, we will prepare our data by selecting the necessary variables. We’ll have to pivot the items data to get it into wide format so we can join with our survivalists data set.\n\n\nCode\nitems_wide <- \n  loadouts |> \n  mutate(item = fct_lump_min(item, min = 3)) |>\n  mutate(present = 1) |> \n  pivot_wider(\n    id_cols = c(name, season), \n    names_from = item, \n    values_from = present,\n    values_fn = first,\n    values_fill = 0,\n    names_prefix = \"item_\"\n  )\n\nalone_clean <-\n  survivalists |> \n  left_join(seasons |> select(season, location)) |> \n  left_join(items_wide) |> \n  mutate(repeat_attempt = rank(season) > 1, .by = name) |> \n  select(\n    name, season, age, gender, location, days_lasted, team, country, profession, repeat_attempt, \n    starts_with(\"item_\")\n  )\n\n\n\nTrain/Test Sets\nNext we will create our train/test sets, as well as some resamples. Since our data set is so small, we will use bootstrap resampling and we will stratify by season to ensure we sample across different survival conditions.\n\n\nCode\nlibrary(tidymodels)\n\nset.seed(234)\nalone_split <- initial_split(alone_clean, strata = season)\nalone_train <- training(alone_split)\nalone_test <- testing(alone_split)\n\nalone_boot <- bootstraps(alone_train, times = 10, strata = season)\n\n\n\n\nFeature Engineering\nMoving on to our feature engineering, we will create a few of the variables we’ve already explored.\nOne of my favorite pre-processing techniques to use when data includes a grouping structure is step_lencode_mixed() from the {embed} package. This step assigns a numeric value to our location variable by training a simple mixed model using the {lme4} package. This is better than dummy encoding because it accounts for differing group counts and will automatically assign a value close to the mean when a new location is encountered. Plus, it doesn’t increase the number of dimensions of the data set as dummy encoding would.\n\n\nCode\nalone_rec <- \n  recipe(\n    days_lasted ~ .,\n    data = alone_train\n  ) |> \n  update_role(name, new_role = \"id\") |> \n  update_role(season, new_role = \"season_id\") |> \n  step_mutate(\n    prior_experience = profession |> stringr::str_detect(\"Surv|Wilderness|Hunt|Outdoor|Skills|Instruct\"),\n    US_orig = country == \"United States\",\n    team = !is.na(team),\n    across(where(is.logical), factor)\n  ) |> \n  embed::step_lencode_mixed(location, outcome = vars(days_lasted)) |> \n  step_rm(profession, country) |> \n  step_dummy(all_factor_predictors())\n\n\n\n\nModel Training/Tuning\nLet’s train a random forest regression model. We will tune and optimize for RMSE. Since it’s a small data set, we will stick with relatively small numbers of trees.\n\n\nCode\nrf_spec <- \n  rand_forest(mtry = tune(), trees = tune()) |>\n  set_engine(\"ranger\") |> \n  set_mode(\"regression\")\n\nalone_wf <-\n  workflow() |>\n  add_recipe(alone_rec) |>\n  add_model(rf_spec)\n\nrf_grid <- \n  grid_regular(\n    mtry(c(5, 25)), \n    trees(c(10, 100)), \n    levels = c(mtry = 5, trees = 10)\n  )\n\n\n\n\nCode\ndoParallel::registerDoParallel()\n\nalone_res <-\n  alone_wf |>\n  tune_grid(\n    resamples = alone_boot,\n    grid = rf_grid,\n    control = control_resamples(allow_par = T, parallel_over = \"everything\")\n  )\n\n\n\n\nCode\nalone_res |> autoplot()\n\n\n\n\n\nTurns out this is a very hard problem to predict. Nevertheless, we will pick our best model and do one last fit.\n\n\nCode\nbest_params <- alone_res |> select_best(metric = \"rmse\")\n\nset.seed(678)\n\nalone_final <- \n  alone_wf |> \n  update_model(rand_forest(mode = \"regression\") |> set_engine(\"ranger\", importance = \"permutation\")) |>\n  finalize_workflow(best_params) |> \n  last_fit(alone_split)"
  },
  {
    "objectID": "posts/alone/index.html#results",
    "href": "posts/alone/index.html#results",
    "title": "Predicting Survival Times from Alone",
    "section": "Results",
    "text": "Results\n\nFinal Performance Metrics\n\n\nCode\nalone_final |> collect_metrics()\n\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard      27.3   Preprocessor1_Model1\n2 rsq     standard       0.186 Preprocessor1_Model1\n\n\nLooks like the final model’s metrics were slightly better than our cross-validated metrics. This is probably because we got to train on the full training set.\n\n\nPredictions\nNow we will compare our predictions against the true values from the test set. The dotted black line represents the formula y = x, which is where we’d hope all our predictions would lie.\n\n\nCode\nalone_final |> \n  collect_predictions() %>%\n  inner_join(alone_clean |> slice(.$.row)) |> \n  ggplot(aes(.pred, days_lasted)) +\n  geom_point(aes(color = location, shape = location), size = 2) +\n  geom_abline(slope = 1, intercept = 0, lty = 2) +\n  labs(shape = NULL, color = NULL, x = \"Predicted Days Lasted\", y = \"Actual Days Lasted\") +\n  expand_limits(x = 0, y = 0)\n\n\n\n\n\nThe good news is that there doesn’t appear to be a whole lot of bias in our model (no systematic under or over predicting). However, our predictions tend to be quite far off most of the time.\n\n\nVariable Importance\nTo finish things off, we’ll look at the variable importances tracked by our model. It’s important to note that permutation importance doesn’t indicate direction of effect, only magnitude. Nonetheless, we can infer the probable direction by looking back at our EDA plots.\n\n\nCode\nalone_final |> \n  extract_fit_engine() |>\n  vip::vip(n = 20)\n\n\n\n\n\nUnsurprisingly we see that gender and location affect survival time. Interestingly, there are a few not so obvious items that appear to affect survival time. Perhaps trapping wire is a must to survive in the wild!"
  },
  {
    "objectID": "posts/alone/index.html#conclusion-discussion",
    "href": "posts/alone/index.html#conclusion-discussion",
    "title": "Predicting Survival Times from Alone",
    "section": "Conclusion & Discussion",
    "text": "Conclusion & Discussion\nAfter exploring and modeling this data set, it’s clear that it’s quite difficult to predict how long contestants will last on the show. There are likely two main reasons for this. The first is that our sample is so limited. 9 seasons with 10 contestants each is no where near enough to extract robust and meaningful patterns. The second reason is that we don’t have enough data on the existing contestants. Additional information that could be helpful include relationship status/family info, personality characteristics, and socio-economic status.\nMore detailed data about the locations could be helpful as well, such as time of year and weather patterns at drop-off time. Perhaps some of this information could be acquired from actually watching the show.\nThere’s also the fact that there is likely some inconsistent bias that the producers of the show introduce when they select contestants. It’s very possible that they select people differently each season based on how viewership and popularity of prior seasons. This makes it difficult to detect potential patterns across seasons."
  },
  {
    "objectID": "posts/big-tech-stock/index.html",
    "href": "posts/big-tech-stock/index.html",
    "title": "Decline of Big Tech Stock",
    "section": "",
    "text": "Welcome to my very first #tidytuesday blog post! In this post I will showcase the new per-operation grouping functionality released in dplyr 1.1.0. I also want to demonstrate one of my favorite lesser-known dplyr tricks!\nFor this analysis, we’ll explore this week’s tidytuesday data set consisting of daily big tech stock prices from 2010-2022.\nFirst, let’s download the data.\n\nlibrary(tidyverse)\n\nbig_tech_stock_prices <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-02-07/big_tech_stock_prices.csv')\nbig_tech_companies <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-02-07/big_tech_companies.csv')\n\nbig_tech_stock_prices |> glimpse()\n\nRows: 45,088\nColumns: 8\n$ stock_symbol <chr> \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \"…\n$ date         <date> 2010-01-04, 2010-01-05, 2010-01-06, 2010-01-07, 2010-01-…\n$ open         <dbl> 7.622500, 7.664286, 7.656429, 7.562500, 7.510714, 7.60000…\n$ high         <dbl> 7.660714, 7.699643, 7.686786, 7.571429, 7.571429, 7.60714…\n$ low          <dbl> 7.585000, 7.616071, 7.526786, 7.466071, 7.466429, 7.44464…\n$ close        <dbl> 7.643214, 7.656429, 7.534643, 7.520714, 7.570714, 7.50392…\n$ adj_close    <dbl> 6.515213, 6.526476, 6.422664, 6.410790, 6.453412, 6.39648…\n$ volume       <dbl> 493729600, 601904800, 552160000, 477131200, 447610800, 46…\n\nbig_tech_stock_prices |> \n  count(stock_symbol) |> \n  inner_join(big_tech_companies) |> \n  select(company, stock_symbol, n)\n\n# A tibble: 14 × 3\n   company                                     stock_symbol     n\n   <chr>                                       <chr>        <int>\n 1 Apple Inc.                                  AAPL          3271\n 2 Adobe Inc.                                  ADBE          3271\n 3 Amazon.com, Inc.                            AMZN          3271\n 4 Salesforce, Inc.                            CRM           3271\n 5 Cisco Systems, Inc.                         CSCO          3271\n 6 Alphabet Inc.                               GOOGL         3271\n 7 International Business Machines Corporation IBM           3271\n 8 Intel Corporation                           INTC          3271\n 9 Meta Platforms, Inc.                        META          2688\n10 Microsoft Corporation                       MSFT          3271\n11 Netflix, Inc.                               NFLX          3271\n12 NVIDIA Corporation                          NVDA          3271\n13 Oracle Corporation                          ORCL          3271\n14 Tesla, Inc.                                 TSLA          3148\n\n\nNote: Meta and Tesla did not go public until after Jan 2010, so they have slightly less data.\nNow let’s visualize the stocks as simple faceted line charts.\n\nbig_tech_stock_prices |>\n  ggplot(aes(date, adj_close)) +\n  geom_line() +\n  facet_wrap(~stock_symbol, ncol = 4, scales = \"free_y\") +\n  labs(x = NULL, y = NULL)\n\n\n\n\nWith the exception of IBM, each stock peaks around the end of 2021, and then declines thereafter.\nLet’s zoom in on the peaks by plotting them on the same axes. Here we will use the new by argument to find the maximum adj_close for each stock_symbol.\n\nbig_tech_stock_prices |> \n  filter(stock_symbol != \"IBM\") |> \n  slice_max(adj_close, by = stock_symbol) |> \n  ggplot(aes(date, adj_close, color = stock_symbol)) +\n  geom_point() +\n  ggrepel::geom_text_repel(aes(label = stock_symbol), size = 3, vjust = -.75) +\n  scale_x_date(labels = scales::label_date_short(), breaks = \"month\") +\n  labs(title = \"Peak Stock Prices\", x = NULL, y = NULL) +\n  theme(legend.position = \"none\")\n\n\n\n\nHow steep of a decline do these stocks see in the weeks and months following their peak? How do they compare to each other?\nTo answer this question, we will need to filter each stock to include only the data following its peak. This may seem trivial at first, but it’s a bit trickier than you might think. Since each stock reaches its peak at a different point, we can’t simply filter the whole data set by a single value.\nOne approach would be to make a separate tibble containing just the max price dates, join it back with the original, and filter the dates.\n\npeak_price_dates <- \n  big_tech_stock_prices |> \n  slice_max(adj_close, by = stock_symbol) |> \n  select(stock_symbol, peak_date = date, peak_price = adj_close)\n\nbig_tech_stock_prices |> \n  inner_join(peak_price_dates) |> \n  filter(date >= peak_date) |> \n  # for demonstration purposes\n  slice_min(date, n = 3, by = stock_symbol) |> \n  select(stock_symbol, date, adj_close)\n\n# A tibble: 42 × 3\n   stock_symbol date       adj_close\n   <chr>        <date>         <dbl>\n 1 AAPL         2022-01-03      181.\n 2 AAPL         2022-01-04      179.\n 3 AAPL         2022-01-05      174.\n 4 ADBE         2021-11-19      688.\n 5 ADBE         2021-11-22      674.\n 6 ADBE         2021-11-23      665.\n 7 AMZN         2021-07-08      187.\n 8 AMZN         2021-07-09      186.\n 9 AMZN         2021-07-12      186.\n10 CRM          2021-11-08      310.\n# … with 32 more rows\n\n\nThis solution works, but there’s a better (in my opinion) way that doesn’t require a separate tibble. The method is derived from a base-R concept called subsetting.\nSubsetting can be used to filter a vector or dataframe by some condition, much like dplyr::filter. Instead of a function call, we use square brackets ([]).\n\nvec <- 0:20\n# subset to get even numbers\nvec[vec %% 2 == 0]\n\n [1]  0  2  4  6  8 10 12 14 16 18 20\n\n# subset to get rows with mpg > 21\nmtcars[mtcars$mpg > 21,]\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nDatsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nMerc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\nIn dplyr, we can apply this same principle by combining a filter with a subset. For our case, we combine filter, subset, and which.max. Coupled with per-operation grouping, we can accomplish the entire process in a single step!\n\nstock_downfalls <- \n  big_tech_stock_prices |>\n  # filter date by the grouped and subsetted date\n  filter(date >= date[which.max(adj_close)], .by = stock_symbol)\n\nstock_downfalls |> \n  ggplot(aes(date, adj_close, color = stock_symbol)) +\n  geom_line() +\n  ggrepel::geom_label_repel(aes(peak_date, peak_price, label = stock_symbol), data = peak_price_dates, size = 3) +\n  scale_x_date(labels = scales::label_date_short(), breaks = \"month\") +\n  labs(x = NULL, y = NULL) +\n  theme(legend.position = \"none\")\n\n\n\n\nNow we can clearly see which stocks endured more dramatic price dips and how they fared over the following 1-2 years.\nThank you so much for reading and I hope this exercise was useful. Please reach out if you have any questions or feedback!"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Predicting Survival Times from Alone\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2023\n\n\nParker Barnes\n\n\n\n\n\n\n  \n\n\n\n\nDecline of Big Tech Stock\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 9, 2023\n\n\nParker Barnes\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "apps/alone_dashboard.html",
    "href": "apps/alone_dashboard.html",
    "title": "Alone",
    "section": "",
    "text": "filter_select(\n  id = \"season\",\n  label = \"Season\",\n  sharedData = sd,\n  group = ~season,\n  multiple = F\n    \n)\n\n\nSeason\n\n\n\n\n\n\n# radioGroupButtons(\n#   \"season\",\n#   \"Season\",\n#   choices = seasons$season,\n#   justified = T\n# )\n\nreactable(\n  sd,\n  columns = list(\n    name = colDef(name = \"Name\", show = T),\n    image_url = colDef(\n      name = \"Profile (click to see more)\",\n      show = T,\n      cell = function(img_url, i) {\n        url <- survivalists |> slice(i) |> pull(url)\n        image <- tags$a(href = str_glue(\"https://www.history.com/shows/alone/cast/{url}\"), \n        img(src = str_glue(\"https://cropper.watch.aetnd.com/cdn.watch.aetnd.com/sites/2/{img_url}\"), height = \"50px\"))\n        tagList(\n          div(style = \"display: inline-block; width: 65px;\", image)\n        )\n    }),\n    reason_tapped_out = colDef(name = \"Reason Tapped Out\"),\n    season = colDef(name = \"Season\", show = T),\n    days_lasted = colDef(name = \"Days Lasted\", show = T),\n    location = colDef(show = F),\n    country = colDef(show = F),\n    lat = colDef(show = F),\n    lng = colDef(show = F)\n  ),\n  details = function(index) {\n    item_data <- loadouts[loadouts$name == alone$name[index],] |> select(Items = item_detailed)\n    htmltools::div(style = \"padding: 1rem\",\n      reactable(item_data, outlined = T))\n  },\n  style = \"overflow: scroll\",\n  pagination = F\n  # height = 250\n)"
  },
  {
    "objectID": "apps/alone_dashboard.html#column-1",
    "href": "apps/alone_dashboard.html#column-1",
    "title": "Alone",
    "section": "Column",
    "text": "Column\n\nMap\n\nsd |> \n  # distinct(season, lat, lng, location, country) |>\n  leaflet() |> \n  addTiles() |> \n  addMarkers(\n    # clusterOptions = markerClusterOptions(),\n    popup = ~str_glue(\"{location}, {country} (Season {season})\")\n    # data = sd\n  )\n\nAssuming \"lng\" and \"lat\" are longitude and latitude, respectively"
  },
  {
    "objectID": "apps/index.html",
    "href": "apps/index.html",
    "title": "Visualizing Alone Survivalists with {crosstalk} and {reactable}",
    "section": "",
    "text": "This is a super basic dashboard showcasing the data I explored in one of my recent blog posts. I mostly wanted to try out a few packages I had heard about but hadn’t used before. {flexdashboard} lets you create dashboards using RMarkdown files, and does not require Shiny. {crosstalk} adds basic interaction between dashboard elements. Again, without using shiny. And finally, {reactable} is a package for creating interactive tables, and it allowed me to incorporate nested tables for the survivalists’ items.\nI specifically used {flexdashboard} and {crosstalk} to avoid having to use Shiny, since it requires a server and a hosting service. I wanted to see if I could embed the dashboard directly in my website. Unfortunately, quarto does not support {flexdashboard} quite yet, but it’s in the works! So for now, I deployed my dashboard on RPubs, which is a free publishing platform supported by Posit.\nYou can access my app here: Alone Dashboard"
  },
  {
    "objectID": "apps.html",
    "href": "apps.html",
    "title": "Shiny Apps",
    "section": "",
    "text": "Visualizing Alone Survivalists with {crosstalk} and {reactable}\n\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2023\n\n\nParker Barnes\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/alone_dashboard/index.html",
    "href": "projects/alone_dashboard/index.html",
    "title": "Visualizing Alone Survivalists with {crosstalk} and {reactable}",
    "section": "",
    "text": "This is a super basic dashboard showcasing the data I explored in one of my recent blog posts. I mostly wanted to try out a few packages I had heard about but hadn’t used before. {flexdashboard} lets you create dashboards using RMarkdown files, and does not require Shiny. {crosstalk} adds basic interaction between dashboard elements. Again, without using shiny. And finally, {reactable} is a package for creating interactive tables, and it allowed me to incorporate nested tables for the survivalists’ items.\nI specifically used {flexdashboard} and {crosstalk} to avoid having to use Shiny, since it requires a server and a hosting service. I wanted to see if I could embed the dashboard directly in my website. Unfortunately, quarto does not support {flexdashboard} quite yet, but it’s in the works! So for now, I deployed my dashboard on RPubs, which is a free publishing platform supported by Posit.\nYou can access my app here: Alone Dashboard"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Visualizing Alone Survivalists with {crosstalk} and {reactable}\n\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2023\n\n\nParker Barnes\n\n\n\n\n\n\nNo matching items"
  }
]